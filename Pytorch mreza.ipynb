{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCEpzyuH9r+CarS5sTUSc/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Azzimihr/ml-lab/blob/main/Pytorch%20mreza.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJNX7oQs8NMf",
        "outputId": "51eb1de2-7abb-4f2f-a710-633e6c254021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Accuracy: 0.6329770610663167\n",
            "1 Accuracy: 0.47409213075817236\n",
            "2 Accuracy: 0.4710919432464529\n",
            "3 Accuracy: 0.5310956934808425\n",
            "4 Accuracy: 0.5536596037252328\n",
            "5 Accuracy: 0.6146009125570349\n",
            "6 Accuracy: 0.6481030064379024\n",
            "7 Accuracy: 0.7037314832177011\n",
            "8 Accuracy: 0.7537346084130259\n",
            "9 Accuracy: 0.790299393712107\n",
            "10 Accuracy: 0.8189261828864304\n",
            "11 Accuracy: 0.8271141946371648\n",
            "12 Accuracy: 0.8489905619101193\n",
            "13 Accuracy: 0.8658666166635415\n",
            "14 Accuracy: 0.8722420151259453\n",
            "15 Accuracy: 0.8743671479467466\n",
            "16 Accuracy: 0.8754297143571473\n",
            "17 Accuracy: 0.8838677417338584\n",
            "18 Accuracy: 0.8932433277079818\n",
            "19 Accuracy: 0.9013063316457278\n",
            "20 Accuracy: 0.9011188199262454\n",
            "21 Accuracy: 0.9033064566535408\n",
            "22 Accuracy: 0.908994312144509\n",
            "23 Accuracy: 0.9134945934120883\n",
            "24 Accuracy: 0.9167447965497844\n",
            "25 Accuracy: 0.918619913744609\n",
            "26 Accuracy: 0.9213075817238577\n",
            "27 Accuracy: 0.9236827301706356\n",
            "28 Accuracy: 0.9244327770485655\n",
            "29 Accuracy: 0.9266829176823551\n",
            "30 Accuracy: 0.928620538783674\n",
            "31 Accuracy: 0.9303081442590162\n",
            "32 Accuracy: 0.9309956872304519\n",
            "33 Accuracy: 0.9323082692668292\n",
            "34 Accuracy: 0.9337458591161948\n",
            "35 Accuracy: 0.9351834489655604\n",
            "36 Accuracy: 0.9371210700668792\n",
            "37 Accuracy: 0.938621163822739\n",
            "38 Accuracy: 0.939496218513657\n",
            "39 Accuracy: 0.9409338083630227\n",
            "40 Accuracy: 0.941871366960435\n",
            "41 Accuracy: 0.9436839802487655\n",
            "42 Accuracy: 0.9448090505656603\n",
            "43 Accuracy: 0.9462466404150259\n",
            "44 Accuracy: 0.9472467029189324\n",
            "45 Accuracy: 0.948684292768298\n",
            "46 Accuracy: 0.9499343708981811\n",
            "47 Accuracy: 0.9502468904306519\n",
            "48 Accuracy: 0.951496968560535\n",
            "49 Accuracy: 0.9519344959059941\n",
            "50 Accuracy: 0.9531845740358772\n",
            "51 Accuracy: 0.9543721482592662\n",
            "52 Accuracy: 0.9549971873242078\n",
            "53 Accuracy: 0.9556847302956435\n",
            "54 Accuracy: 0.956309769360585\n",
            "55 Accuracy: 0.9571848240515032\n",
            "56 Accuracy: 0.9581223826489156\n",
            "57 Accuracy: 0.9587474217138571\n",
            "58 Accuracy: 0.9591224451528221\n",
            "59 Accuracy: 0.9592474529658104\n",
            "60 Accuracy: 0.9599349959372461\n",
            "61 Accuracy: 0.9611850740671292\n",
            "62 Accuracy: 0.9609350584411526\n",
            "63 Accuracy: 0.9618726170385649\n",
            "64 Accuracy: 0.9623726482905182\n",
            "65 Accuracy: 0.9628726795424714\n",
            "66 Accuracy: 0.9638102381398838\n",
            "67 Accuracy: 0.9641227576723546\n",
            "68 Accuracy: 0.9644977811113195\n",
            "69 Accuracy: 0.9649353084567786\n",
            "70 Accuracy: 0.9655603475217202\n",
            "71 Accuracy: 0.9660603787736733\n",
            "72 Accuracy: 0.966747921745109\n",
            "73 Accuracy: 0.9668729295580973\n",
            "74 Accuracy: 0.9672479529970623\n",
            "75 Accuracy: 0.9680605037814863\n",
            "76 Accuracy: 0.9684355272204512\n",
            "77 Accuracy: 0.9686230389399337\n",
            "78 Accuracy: 0.9689355584724045\n",
            "79 Accuracy: 0.9688730545659103\n",
            "80 Accuracy: 0.9690605662853928\n",
            "81 Accuracy: 0.9694980936308519\n",
            "82 Accuracy: 0.9702481405087818\n",
            "83 Accuracy: 0.9706856678542409\n",
            "84 Accuracy: 0.9708106756672292\n",
            "85 Accuracy: 0.9711856991061941\n",
            "86 Accuracy: 0.9714982186386649\n",
            "87 Accuracy: 0.9716857303581474\n",
            "88 Accuracy: 0.9719982498906181\n",
            "89 Accuracy: 0.9724357772360772\n",
            "90 Accuracy: 0.9726857928620539\n",
            "91 Accuracy: 0.9726857928620539\n",
            "92 Accuracy: 0.9730608163010188\n",
            "93 Accuracy: 0.9734983436464779\n",
            "94 Accuracy: 0.9734358397399837\n",
            "95 Accuracy: 0.9738108631789487\n",
            "96 Accuracy: 0.9743108944309019\n",
            "97 Accuracy: 0.9746859178698669\n",
            "98 Accuracy: 0.9750609413088318\n",
            "99 Accuracy: 0.9751859491218201\n"
          ]
        }
      ],
      "source": [
        "import torch as t\n",
        "from torch.nn import *\n",
        "from numpy import array\n",
        "from pandas import read_csv\n",
        "from torch.nn.functional import cross_entropy, one_hot\n",
        "from sklearn.model_selection import train_test_split as split\n",
        "\n",
        "train,test = split( read_csv(\"sample_data/mnist_train_small.csv\").rename(columns={\"6\": \"label\"}), test_size=0.2)\n",
        "train = array(train).T\n",
        "# test = array(test).T\n",
        "\n",
        "x = t.from_numpy((train[1:] / 255).T).float()\n",
        "y = t.from_numpy(train[0]).long()\n",
        "ohy = one_hot(y.to(t.int64), 10).float()\n",
        "\n",
        "class Model(Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net = Sequential (Linear(784,200), ReLU(), Dropout(p=0.5), Linear(200,200), Dropout(p=0.5),  Linear(200,10), Softmax())\n",
        "\n",
        "model=Model()\n",
        "opt=t.optim.Adam(model.parameters(), lr=1e-2)\n",
        "\n",
        "for i in range(100):\n",
        "  model.train()\n",
        "  cross_entropy(model.net(x),ohy).backward() #loss\n",
        "  opt.step()\n",
        "  opt.zero_grad()\n",
        "  model.eval()\n",
        "  print(i,\"Accuracy:\",t.sum(t.argmax(model.net(x), dim=1)==y).item()/len(y))\n"
      ]
    }
  ]
}