{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Azzimihr/ml-lab/blob/main/Pytorch%20mreza.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "vvopt6oHjMEK",
        "outputId": "8dc74e24-375e-475c-ab55-529ea65bd132",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Accuracy: 0.2633914619663729\n",
            "1 Accuracy: 0.3964622788924308\n",
            "2 Accuracy: 0.47971748234264644\n",
            "3 Accuracy: 0.5337833614600913\n",
            "4 Accuracy: 0.5682855178448653\n",
            "5 Accuracy: 0.5897243577723608\n",
            "6 Accuracy: 0.6019126195387212\n",
            "7 Accuracy: 0.6120382523907745\n",
            "8 Accuracy: 0.6184761547596724\n",
            "9 Accuracy: 0.6235389711856991\n",
            "10 Accuracy: 0.6279767485467842\n",
            "11 Accuracy: 0.633914619663729\n",
            "12 Accuracy: 0.6383523970248141\n",
            "13 Accuracy: 0.6450403150196887\n",
            "14 Accuracy: 0.6517907369210576\n",
            "15 Accuracy: 0.661166322895181\n",
            "16 Accuracy: 0.668416776048503\n",
            "17 Accuracy: 0.6745421588849303\n",
            "18 Accuracy: 0.6806675417213576\n",
            "19 Accuracy: 0.6848553034564661\n",
            "20 Accuracy: 0.688480530033127\n",
            "21 Accuracy: 0.6924182761422589\n",
            "22 Accuracy: 0.6962310144384024\n",
            "23 Accuracy: 0.7009813113319583\n",
            "24 Accuracy: 0.7033564597787362\n",
            "25 Accuracy: 0.7059191199449966\n",
            "26 Accuracy: 0.7086067879242453\n",
            "27 Accuracy: 0.7105444090255641\n",
            "28 Accuracy: 0.7127320457528595\n",
            "29 Accuracy: 0.71454465904119\n",
            "30 Accuracy: 0.716544784049003\n",
            "31 Accuracy: 0.7183573973373336\n",
            "32 Accuracy: 0.7194824676542284\n",
            "33 Accuracy: 0.7227326707919245\n",
            "34 Accuracy: 0.7259203700231265\n",
            "35 Accuracy: 0.7323582723920246\n",
            "36 Accuracy: 0.741796362272642\n",
            "37 Accuracy: 0.7558597412338272\n",
            "38 Accuracy: 0.7731108194262142\n",
            "39 Accuracy: 0.7852365772860804\n",
            "40 Accuracy: 0.7924245265329083\n",
            "41 Accuracy: 0.7953622101381337\n",
            "42 Accuracy: 0.7977998624914057\n",
            "43 Accuracy: 0.7995499718732421\n",
            "44 Accuracy: 0.8013000812550785\n",
            "45 Accuracy: 0.8039252453278329\n",
            "46 Accuracy: 0.805550346896681\n",
            "47 Accuracy: 0.8070504406525408\n",
            "48 Accuracy: 0.8113007062941434\n",
            "49 Accuracy: 0.8177386086630415\n",
            "50 Accuracy: 0.8253640852553285\n",
            "51 Accuracy: 0.8347396712294518\n",
            "52 Accuracy: 0.8450528158009876\n",
            "53 Accuracy: 0.8526157884867804\n",
            "54 Accuracy: 0.860053753359585\n",
            "55 Accuracy: 0.8633664604037752\n",
            "56 Accuracy: 0.8668041752609538\n",
            "57 Accuracy: 0.8698668666791675\n",
            "58 Accuracy: 0.871679479967498\n",
            "59 Accuracy: 0.8714294643415214\n",
            "60 Accuracy: 0.8712419526220389\n",
            "61 Accuracy: 0.872117007312957\n",
            "62 Accuracy: 0.8732420776298518\n",
            "63 Accuracy: 0.8756797299831239\n",
            "64 Accuracy: 0.8781798862428902\n",
            "65 Accuracy: 0.8819926245390337\n",
            "66 Accuracy: 0.8851178198637415\n",
            "67 Accuracy: 0.8884930308144259\n",
            "68 Accuracy: 0.8916807300456279\n",
            "69 Accuracy: 0.8943058941183823\n",
            "70 Accuracy: 0.8956184761547596\n",
            "71 Accuracy: 0.8966185386586661\n",
            "72 Accuracy: 0.8974935933495843\n",
            "73 Accuracy: 0.8980561285080317\n",
            "74 Accuracy: 0.8988686792924557\n",
            "75 Accuracy: 0.899368710544409\n",
            "76 Accuracy: 0.8997437339833739\n",
            "77 Accuracy: 0.8999937496093506\n",
            "78 Accuracy: 0.900556284767798\n",
            "79 Accuracy: 0.9011188199262454\n",
            "80 Accuracy: 0.902181386336646\n",
            "81 Accuracy: 0.9032439527470467\n",
            "82 Accuracy: 0.904181511344459\n",
            "83 Accuracy: 0.9046190386899181\n",
            "84 Accuracy: 0.9055565972873305\n",
            "85 Accuracy: 0.9063691480717545\n",
            "86 Accuracy: 0.9072442027626727\n",
            "87 Accuracy: 0.9079317457341084\n",
            "88 Accuracy: 0.9085567847990499\n",
            "89 Accuracy: 0.9094943433964623\n",
            "90 Accuracy: 0.9098068629289331\n",
            "91 Accuracy: 0.910181886367898\n",
            "92 Accuracy: 0.9105569098068629\n",
            "93 Accuracy: 0.911369460591287\n",
            "94 Accuracy: 0.911369460591287\n",
            "95 Accuracy: 0.9118069879367461\n",
            "96 Accuracy: 0.9119319957497344\n",
            "97 Accuracy: 0.9124945309081818\n",
            "98 Accuracy: 0.9126195387211701\n",
            "99 Accuracy: 0.912994562160135\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "import torch as t\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.nn import *\n",
        "from sklearn.model_selection import train_test_split as split\n",
        "\n",
        "train, test = split( pd.read_csv(\"sample_data/mnist_train_small.csv\").rename(columns={\"6\": \"label\"}), test_size=0.2)\n",
        "train = np.array(train).T\n",
        "#test = array(test).T\n",
        "\n",
        "X = t.from_numpy((train[1:] / 255).T).float()\n",
        "Y = t.from_numpy(train[0]).long()\n",
        "\n",
        "class Model(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = Sequential( Linear(784, 150), ReLU(), Linear(150, 10), Softmax() )\n",
        "    def pred(self, x):\n",
        "        return t.argmax( self.net(x), dim=1)\n",
        "\n",
        "model=Model()\n",
        "\n",
        "def fit(loss_fn, opt):\n",
        "    ohy = F.one_hot(Y.to(t.int64), 10).float()\n",
        "    for i in range(100):\n",
        "        loss_fn(model.net(X), ohy).backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        print(i, \"Accuracy:\", t.sum(model.pred(X) == Y).item() / len(Y))\n",
        "\n",
        "fit(F.cross_entropy, t.optim.Adam(model.parameters(), lr=1e-3) )"
      ]
    }
  ]
}